{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DisasterTweets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk8CU3-AAuvi",
        "outputId": "2f933fd6-aae8-4cf4-a336-5866e626ee5d"
      },
      "source": [
        "pip install contractions wordninja"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/5f/91102df95715fdda07f56a7eba2baae983e2ae16a080eb52d79e08ec6259/contractions-0.0.45-py2.py3-none-any.whl\n",
            "Collecting wordninja\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/15/abe4af50f4be92b60c25e43c1c64d08453b51e46c32981d80b3aebec0260/wordninja-2.0.0.tar.gz (541kB)\n",
            "\r\u001b[K     |‚ñã                               | 10kB 3.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñè                              | 20kB 5.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñâ                              | 30kB 6.1MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñç                             | 40kB 5.7MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà                             | 51kB 6.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñã                            | 61kB 7.0MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñé                           | 71kB 6.9MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 81kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 92kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 102kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 112kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 122kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 133kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 143kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 153kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 163kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 174kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 184kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 194kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 204kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 215kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 225kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 235kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 245kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 256kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 266kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 276kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 286kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 296kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 307kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 317kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 327kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 337kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 348kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 358kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 368kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 378kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 389kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 399kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 409kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 419kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 430kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 440kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 450kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 460kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 471kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 481kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 491kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 501kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 512kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 522kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 532kB 7.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 542kB 7.6MB/s \n",
            "\u001b[?25hCollecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/92/b3c70b8cf2b76f7e3e8b7243d6f06f7cb3bab6ada237b1bce57604c5c519/pyahocorasick-1.4.1.tar.gz (321kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 327kB 14.7MB/s \n",
            "\u001b[?25hCollecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/65/91eab655041e9e92f948cb7302e54962035762ce7b518272ed9d6b269e93/Unidecode-1.1.2-py2.py3-none-any.whl (239kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 245kB 18.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: wordninja, pyahocorasick\n",
            "  Building wheel for wordninja (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordninja: filename=wordninja-2.0.0-cp36-none-any.whl size=541553 sha256=b6870b49eee16e4105b15c28e6311ee77e0dc02e5ce0560ad4e479f895a75c11\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/46/06/9b6d10ed02c85e93c3bb33ac50e2d368b2586248f192a2e22a\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.1-cp36-cp36m-linux_x86_64.whl size=84340 sha256=89380193a42561d99a014f00bfe04d04435cfbaa5cb605d9ed9924fe981b82b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/ab/f7/cb39270df8f6126f3dd4c33d302357167086db460968cfc80c\n",
            "Successfully built wordninja pyahocorasick\n",
            "Installing collected packages: pyahocorasick, Unidecode, textsearch, contractions, wordninja\n",
            "Successfully installed Unidecode-1.1.2 contractions-0.0.45 pyahocorasick-1.4.1 textsearch-0.0.17 wordninja-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AUxmsanh8lR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d4f34fa-3fc1-46ef-b510-b950ff52200c"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import tweepy\n",
        "import time\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import wordnet\n",
        "from string import punctuation\n",
        "import contractions\n",
        "import re\n",
        "from collections import Counter\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import random\n",
        "import wordninja\n",
        "import math\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "pd.set_option('display.max_colwidth',1000)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrt64JFoMtZQ"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmJOkoK8Rb94"
      },
      "source": [
        "class TweetPreprocessing:\n",
        "    def __init__(self, word_normalization):\n",
        "      self.tokenizer = TweetTokenizer()\n",
        "      self.stop_words = set(stopwords.words('english'))\n",
        "      self.lemmatizer = WordNetLemmatizer()\n",
        "      self.stemmer = PorterStemmer()\n",
        "      self.ignore_words = ['..', '...', '‚Äô', '‚Äò', '‚Ä¶', '‚Äú', '‚Äù', '', '‚†Ä', '\\x89', '√ª_']\n",
        "\n",
        "      if word_normalization in ['stemming', 'basic_lemmatization', 'pos_lemmatization']:\n",
        "        self.word_normalization = word_normalization\n",
        "      else:\n",
        "        print('word_normalization parameter defaults to stemming')\n",
        "        self.word_normalization = 'stemming'\n",
        "      \n",
        "    def unnest_lists(self, l):\n",
        "      \"\"\"\n",
        "      Returns a flattened list from a list of tokens that may contain nested lists.\n",
        "      Parameters:\n",
        "          l: a list of tokens, some of which might be lists themselves\n",
        "      \"\"\"\n",
        "      flattened_list = []\n",
        "      for item in l: \n",
        "        if type(item) == list:\n",
        "          for nested_item in item:\n",
        "            flattened_list.append(nested_item)\n",
        "        else: \n",
        "          flattened_list.append(item)\n",
        "      \n",
        "      return flattened_list\n",
        "      \n",
        "    def get_document_pos(self, tokenized_document):\n",
        "      \"\"\"\n",
        "      Returns a document in the form of (word, tag) where each word is tupled with one of four tags that can be applied to lemmatization afterwards.\n",
        "      Parameters:\n",
        "          tokenized_document: a list of tokens\n",
        "      \"\"\"\n",
        "      tagged_doc = nltk.pos_tag(tokenized_document)\n",
        "      tag_dict = {\"J\": wordnet.ADJ,\n",
        "                  \"N\": wordnet.NOUN,\n",
        "                  \"V\": wordnet.VERB,\n",
        "                  \"R\": wordnet.ADV}\n",
        "      \n",
        "      document_pos = [(tagged_word[0], tag_dict.get(tagged_word[1][0], wordnet.NOUN)) for tagged_word in tagged_doc]\n",
        "\n",
        "      return document_pos\n",
        "\n",
        "    def replace_hyperlinks(self, document):\n",
        "      \"\"\"\n",
        "      Returns a copy of the document with removed hyperlinks and leading, trailing whitespaces removed.\n",
        "      Parameters:\n",
        "          document: a string\n",
        "      \"\"\"\n",
        "      return re.sub(r\"http\\S+\", \"\", document).strip()\n",
        "    \n",
        "    def preprocess_dataset(self, data):\n",
        "      \"\"\"\n",
        "      Returns a copy of the original dataset with additional columns: text_preprocessed, duplicate_row.\n",
        "      Parameters:\n",
        "          data: a dataframe including columns for id, tweet text, target class\n",
        "      \"\"\"\n",
        "\n",
        "      # remove hyperlinks and trim leading & trailing whitespaces\n",
        "      data_preprocessed = data[['id', 'text', 'target']].copy()\n",
        "      data_preprocessed['text_preprocessed'] = data_preprocessed['text'].apply(self.replace_hyperlinks)\n",
        "\n",
        "      # mark duplicated rows\n",
        "      data_preprocessed['duplicate_row'] = data_preprocessed.duplicated(subset=['text_preprocessed'], keep='first')\n",
        "      \n",
        "      return data_preprocessed\n",
        "\n",
        "    def preprocess_document(self, document):\n",
        "      \"\"\"\n",
        "      Returns array of tokens that correspond to the preprocessed document.\n",
        "      Parameters:\n",
        "          document: a string\n",
        "      \"\"\"\n",
        "\n",
        "      # expand contractions (for example you`re => you are)\n",
        "      doc_no_contractions = contractions.fix(document)\n",
        "\n",
        "      # tokenize, make lower case and remove punctuation\n",
        "      words = self.tokenizer.tokenize(doc_no_contractions)\n",
        "      doc_no_punct = [w.lower() for w in words if w not in punctuation]\n",
        "\n",
        "      # handle hashtags\n",
        "      doc_no_hashtags = self.unnest_lists([wordninja.split(w[1:]) if w.startswith('#') else w for w in doc_no_punct])\n",
        "\n",
        "      # lemmatization/stemming & stop words removal\n",
        "      tokens_to_ignore = self.stop_words.union(set(self.ignore_words))\n",
        "      \n",
        "      doc_normalized = doc_no_hashtags\n",
        "      if self.word_normalization == 'stemming':\n",
        "        doc_normalized = [self.stemmer.stem(w) for w in doc_normalized if w not in tokens_to_ignore]\n",
        "      elif self.word_normalization == 'basic_lemmatization':\n",
        "        doc_normalized = [self.lemmatizer.lemmatize(w) for w in doc_normalized if w not in tokens_to_ignore]\n",
        "      elif self.word_normalization == 'pos_lemmatization':\n",
        "        doc_pos = self.get_document_pos(doc_normalized)\n",
        "        doc_normalized = [self.lemmatizer.lemmatize(w, t) for (w, t) in doc_pos if w not in tokens_to_ignore]\n",
        "      \n",
        "      return doc_normalized\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ8S3IePfClw"
      },
      "source": [
        "## Feature Ranking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olEGtqVfd304"
      },
      "source": [
        "class FeatureRanking:\r\n",
        "  def __init__(self, preprocesser, target_class=1):\r\n",
        "    self.preprocesser = preprocesser\r\n",
        "    self.target_class = target_class\r\n",
        "    \r\n",
        "    self.mi_rank = {} # dictionary to store mutual information ranking\r\n",
        "    self.chi_sq_rank = {} # dictionary to chi square ranking\r\n",
        "\r\n",
        "  def calculate(self, data, allow_duplicates=True):\r\n",
        "    \"\"\"\r\n",
        "    Calculates mutual information and chi square for each word for the target class (self.target_class).\r\n",
        "    Parameters:\r\n",
        "        data: a dataframe including columns for id, tweet text, target class\r\n",
        "        allow_duplicates: flag whether to include duplicate documents in calculations\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # extract distinct classes\r\n",
        "    classes = data['target'].unique()\r\n",
        "\r\n",
        "    # preprocess the dataset\r\n",
        "    data_preprocessed = self.preprocesser.preprocess_dataset(data)\r\n",
        "    # preprocess each document, including tokenization\r\n",
        "    data_tokenized = [(row['id'], self.preprocesser.preprocess_document(row['text_preprocessed']), row['target'])\r\n",
        "      for index, row in data_preprocessed.iterrows() if allow_duplicates or not row['duplicate_row']]\r\n",
        "\r\n",
        "    N = len(data_tokenized) # number of all documents\r\n",
        "    N_by_class = {} # number of documents for each class\r\n",
        "    word_doc_counts = Counter() # number of documents each word occurs in\r\n",
        "    word_doc_counts_tc = Counter() # number of documents of the target class each word occurs in\r\n",
        "\r\n",
        "    dictionary = set()\r\n",
        "    # populate dictionary of words\r\n",
        "    for doc in data_tokenized:\r\n",
        "      for word in doc[1]:\r\n",
        "        dictionary.add(word)\r\n",
        "\r\n",
        "    for cl in classes:\r\n",
        "      N_by_class[cl] = 0\r\n",
        "\r\n",
        "    for doc in data_tokenized:\r\n",
        "      cl = doc[2] # the class of the document\r\n",
        "      \r\n",
        "      # increment the number of documents of this class\r\n",
        "      N_by_class[cl] += 1\r\n",
        "      \r\n",
        "      # update overall word-document occurrences\r\n",
        "      word_doc_counts.update(set(doc[1]))\r\n",
        "      if cl == self.target_class:\r\n",
        "        # update word-document occurrences for this class\r\n",
        "        word_doc_counts_tc.update(set(doc[1]))\r\n",
        "\r\n",
        "    for word in dictionary:\r\n",
        "      o11 = word_doc_counts_tc[word] # number of documents that contain the word and are of the target class\r\n",
        "      o10 = word_doc_counts[word] - word_doc_counts_tc[word] # number of documents that contain the word and are not of the target class\r\n",
        "      o01 = N_by_class[self.target_class] - o11 # number of documents that don`t contain the word and are of the target class\r\n",
        "      o00 = N - N_by_class[self.target_class] - o10 # number of documents that don`t contain the word and are not of the target class\r\n",
        "\r\n",
        "      o1_ = word_doc_counts[word] # number of documents that contain the word regardless of document class\r\n",
        "      o0_ = N - word_doc_counts[word] # number of documents that don`t contain the word regardless of document class\r\n",
        "      o_1 = N_by_class[self.target_class] # number of documents of the target class\r\n",
        "      o_0 = N - N_by_class[self.target_class] # number of documents not of the target class\r\n",
        "\r\n",
        "      # calculate mutual information\r\n",
        "      self.mi_rank[word] = 0 if o11 == 0 else (o11/N)*math.log2(N*o11/(o1_*o_1)) + \\\r\n",
        "                           (o01/N)*math.log2(N*o01/(o0_*o_1)) + \\\r\n",
        "                           0 if o10 == 0 else (o10/N)*math.log2(N*o10/(o1_*o_0)) + \\\r\n",
        "                           (o00/N)*math.log2(N*o00/(o0_*o_0))\r\n",
        "      \r\n",
        "      # calculate chi square\r\n",
        "      self.chi_sq_rank[word] = ((o11 + o10 + o01 + o00)*(o11*o00 - o10*o01)**2) / \\\r\n",
        "                                 ((o11 + o01)*(o11 + o10)*(o10 + o00)*(o01 + o00))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8shrKHtM5tG"
      },
      "source": [
        "## Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKphLmkzhORh"
      },
      "source": [
        "class NaiveBayesDocumentClassifier:\n",
        "  def __init__(self, preprocesser, vocabulary=None):\n",
        "    self.dictionary = set() # dictionary of all tokens/words in the collection\n",
        "    self.P_Vs = {} # prior target class probabilities\n",
        "    self.P_words = {} # posterior word-class probabilities\n",
        "    self.preprocesser = preprocesser\n",
        "    self.vocabulary = vocabulary\n",
        "  \n",
        "  def fit(self, data, allow_duplicates=True):\n",
        "    \"\"\"\n",
        "    Learns probabilites P(Wk|Vj) that describe the chance of a randomly selected word from a document classified as Vj to be the word Wk.\n",
        "    Parameters:\n",
        "        data: a dataframe including columns for id, tweet text, target class\n",
        "        allow_duplicates: flag whether to include duplicate documents in calculations\n",
        "    \"\"\"\n",
        "    \n",
        "    # preprocess the dataset\n",
        "    data_preprocessed = self.preprocesser.preprocess_dataset(data)\n",
        "    # preprocess each document, including tokenization\n",
        "    data_tokenized = [(row['id'], self.preprocesser.preprocess_document(row['text_preprocessed']), row['target'])\n",
        "      for index, row in data_preprocessed.iterrows() if allow_duplicates or not row['duplicate_row']]\n",
        "    \n",
        "    # populate dictionary of words\n",
        "    if self.vocabulary:\n",
        "      self.dictionary = set(self.vocabulary)\n",
        "    else:\n",
        "      for doc in data_tokenized:\n",
        "        for word in doc[1]:\n",
        "          self.dictionary.add(word)\n",
        "\n",
        "    N = len(data_tokenized) # number of all training examples (documents)\n",
        "    n_dict = len(self.dictionary) # number of words in dictionary\n",
        "\n",
        "    # iterate each class\n",
        "    for V in data_preprocessed['target'].unique():\n",
        "      docs_V = [d for d in data_tokenized if d[2] == V] # the set of documents of class V\n",
        "      self.P_Vs[V] = len(docs_V)/N # calculate priori target class probabilities\n",
        "\n",
        "      text_V = [word for doc in docs_V for word in doc[1]] # union of all documents of the class\n",
        "\n",
        "      n_V = len(text_V) # number of all the words in text_V\n",
        "      text_V_counts = Counter(text_V) # number of occurrences of each word in text_V\n",
        "\n",
        "      for word in self.dictionary:\n",
        "        n_word = text_V_counts[word] # number of occurrences of word in text_V\n",
        "        self.P_words[(word, V)] = (n_word + 1)/(n_V + n_dict) # calculate probability of the word being of class V (Laplace smoothing included)\n",
        "\n",
        "\n",
        "  def predict(self, document):\n",
        "    \"\"\"\n",
        "    Predicts the class of a document.\n",
        "    Parameters:\n",
        "        document: a string that contains the document\n",
        "    \"\"\"\n",
        "    \n",
        "    # preprocess document\n",
        "    doc_no_hyperlinks = self.preprocesser.replace_hyperlinks(document)\n",
        "    doc_tokenized = self.preprocesser.preprocess_document(doc_no_hyperlinks)\n",
        "\n",
        "    # store words from document that exist in classifier`s dictionary\n",
        "    words_in_dictionary = [word for word in doc_tokenized if word in self.dictionary]\n",
        "\n",
        "    p_V = 0\n",
        "    max_p_V = 0\n",
        "    output_V = -1\n",
        "\n",
        "    # calculate probabilities for each class and store the final classification in output_V\n",
        "    for V in self.P_Vs:\n",
        "      p_word_V = 1\n",
        "\n",
        "      # calculate the product of the probabilities of each word in the document to occur under class V\n",
        "      for word in words_in_dictionary:\n",
        "        p_word_V = p_word_V*self.P_words[(word, V)]\n",
        "      \n",
        "      # multiply the prior probability for the class with the posterior probability for each word\n",
        "      p_V = self.P_Vs[V]*p_word_V\n",
        "      \n",
        "      # store the highest class probability\n",
        "      if (p_V > max_p_V):\n",
        "        max_p_V = p_V\n",
        "        output_V = V\n",
        "    \n",
        "    return output_V\n",
        "\n",
        "  def predict_proba(self, document):\n",
        "    \"\"\"\n",
        "    Returns the probabilities of the document being of each class.\n",
        "    Parameters:\n",
        "        document: a string that contains the document\n",
        "    \"\"\"\n",
        "    \n",
        "    # preprocess document\n",
        "    doc_no_hyperlinks = self.preprocesser.replace_hyperlinks(document)\n",
        "    doc_tokenized = self.preprocesser.preprocess_document(doc_no_hyperlinks)\n",
        "\n",
        "    # store words from document that exist in classifier`s dictionary\n",
        "    words_in_dictionary = [word for word in doc_tokenized if word in self.dictionary]\n",
        "\n",
        "    p_V = 0\n",
        "    max_p_V = 0\n",
        "    output_V = -1\n",
        "    output_probs = []\n",
        "\n",
        "    # calculate probabilities for each class and store the final classification in output_V\n",
        "    for V in sorted(nb.P_Vs.keys()):\n",
        "      p_word_V = 1\n",
        "\n",
        "      # calculate the product of the probabilities of each word in the document to occur under class V\n",
        "      for word in words_in_dictionary:\n",
        "        p_word_V = p_word_V*self.P_words[(word, V)]\n",
        "      \n",
        "      # multiply the prior probability for the class with the posterior probability for each word\n",
        "      p_V = self.P_Vs[V]*p_word_V\n",
        "      \n",
        "      # store the highest class probability\n",
        "      if (p_V > max_p_V):\n",
        "        max_p_V = p_V\n",
        "        output_V = V\n",
        "      \n",
        "      # store probability\n",
        "      output_probs.append(p_V)\n",
        "    \n",
        "    return output_probs/sum(output_probs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fqUreSpNJFe"
      },
      "source": [
        "## Classification stats - accuracy, precision, recall, F-score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFppajp7gM1N"
      },
      "source": [
        "class ClassificationStats:\n",
        "    def __init__(self, y_true, y_pred):\n",
        "      self.cm = confusion_matrix(y_true, y_pred)\n",
        "      self.tp = self.cm[1,1]\n",
        "      self.tn = self.cm[0,0]\n",
        "      self.fp = self.cm[0,1]\n",
        "      self.fn = self.cm[1,0]\n",
        "\n",
        "    def precision(self):\n",
        "      \"\"\"\n",
        "      Returns the number of examples correctly classified as True, divided by the number of all examples classified as True.\n",
        "      \"\"\"\n",
        "      return self.tp/(self.tp + self.fp)\n",
        "\n",
        "    def recall(self):\n",
        "      \"\"\"\n",
        "      Returns the number of examples correctly classified as True, divided by the number of all True examples.\n",
        "      \"\"\"\n",
        "      return self.tp/(self.tp + self.fn)\n",
        "\n",
        "    def accuracy(self):\n",
        "      \"\"\"\n",
        "      Returns the number of correctly classified examples, divided by the number of all examples.\n",
        "      \"\"\"\n",
        "      return (self.tp + self.tn)/(self.tp + self.fp + self.tn + self.fn)\n",
        "\n",
        "    def f_measure(self, beta=1):\n",
        "      \"\"\"\n",
        "      Returns the harmonic mean of precision and recall.\n",
        "      Parameters:\n",
        "          beta: controls the weighting of precision versus recall. For example, values of beta < 1 emphasize precision, while beta > 1 emphasize recall.\n",
        "      \"\"\"\n",
        "      return ((beta**2 + 1)*self.precision()*self.recall()) / (beta**2*self.precision() + self.recall())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r7yXjKcNFSe"
      },
      "source": [
        "## Document similarity search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xW5GfcijDJk"
      },
      "source": [
        "class DocumentSimilaritySearch:\r\n",
        "  def __init__(self):\r\n",
        "    self.dictionary = None\r\n",
        "    self.index = None\r\n",
        "\r\n",
        "    self.stop_words = stopwords.words('english')\r\n",
        "    self.ignore_words = ['..', '...', '‚Äô', '‚Äò', '‚Ä¶', '‚Äú', '‚Äù', '', '‚†Ä', '\\x89', '√ª_']\r\n",
        "\r\n",
        "    self.vectorizer = TfidfVectorizer(lowercase=True, analyzer='word', stop_words=self.stop_words+self.ignore_words)\r\n",
        "\r\n",
        "  def create_index(self, data, text_column='text_preprocessed'):\r\n",
        "    \"\"\"\r\n",
        "    Calls fit() method of the vectorizer.\r\n",
        "    Parameters:\r\n",
        "        data: a dataframe in which text_column contains the data to be used for dictionary\r\n",
        "    \"\"\"\r\n",
        "    self.dictionary = data[text_column]\r\n",
        "    self.index = self.vectorizer.fit_transform(data[text_column])\r\n",
        "\r\n",
        "  def find_similar_documents(self, query, n_documents):\r\n",
        "    \"\"\"\r\n",
        "    Returns the top n_documents from the dictionary that are most similar to the query.\r\n",
        "    Parameters:\r\n",
        "        query: a string containing the query\r\n",
        "        n_documents: number of documents to return\r\n",
        "    \"\"\"\r\n",
        "    query_vec = self.vectorizer.transform([query])\r\n",
        "    results = cosine_similarity(self.index, query_vec)\r\n",
        "\r\n",
        "    top_n_indexes = [i[0] for i in np.argsort(results, axis=0)[-n_documents:][::-1]]\r\n",
        "\r\n",
        "    return self.dictionary[top_n_indexes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4iB6-tpNSUd"
      },
      "source": [
        "## Twitter streaming with classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPGQOwNhEexf"
      },
      "source": [
        "class TwitterClassificationStreamListener(tweepy.StreamListener):\n",
        "    def __init__(self, api, time_limit=60, classifier=None, prob_threshold=0):\n",
        "        self.api = api\n",
        "        self.me = api.me()\n",
        "        self.tweets_list = []\n",
        "        self.classifier = classifier\n",
        "        self.prob_threshold = prob_threshold\n",
        "\n",
        "        self.start_time = time.time()\n",
        "        self.limit = time_limit\n",
        "\n",
        "        super(TwitterClassificationStreamListener, self).__init__()\n",
        "\n",
        "    def on_status(self, tweet):\n",
        "      \"\"\"\n",
        "      Wait for Twitter posts and store each post in tweets_list. Run for the specified time limit.\n",
        "      \"\"\"\n",
        "      if (time.time() - self.start_time) < self.limit:\n",
        "        #print(tweet)\n",
        "        if self.classifier:\n",
        "          if self.prob_threshold > 0:\n",
        "            prediction = self.classifier.predict_proba(tweet.text)\n",
        "            if(prediction[1] >= self.prob_threshold): print(tweet.text)\n",
        "          else:\n",
        "            prediction = self.classifier.predict(tweet.text)            \n",
        "            if(prediction == 1): print(tweet.text)\n",
        "\n",
        "          self.tweets_list.append((tweet.text, prediction))\n",
        "        return True\n",
        "      else:\n",
        "        return False\n",
        "\n",
        "    def on_error(self, status):\n",
        "        print(\"Error detected\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdyWMch9Ng22"
      },
      "source": [
        "## Load data from file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "WaV2VzxvhSqW",
        "outputId": "65cefd75-c5ac-4771-fefa-cd7318151ee8"
      },
      "source": [
        "# load data from file\n",
        "filepath = '/content/sample_data/train.csv'\n",
        "data = pd.read_csv(filepath)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ... target\n",
              "0   1  ...      1\n",
              "1   4  ...      1\n",
              "2   5  ...      1\n",
              "3   6  ...      1\n",
              "4   7  ...      1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci3FAx-RNy8H"
      },
      "source": [
        "## Examples for the presentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJ8dybtBFWdh",
        "outputId": "1d41fe8e-fdcf-41de-c45d-8ec3251135ad"
      },
      "source": [
        "data.groupby(['target']).target.count()/len(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "target\n",
              "0    0.57034\n",
              "1    0.42966\n",
              "Name: target, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RUQtwfGvERO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "cab4a455-5e8e-4b4a-a4be-7430adf39e29"
      },
      "source": [
        "### File tweets.csv\r\n",
        "### show examples of different disaster tweets, non-disaster tweets, strange data, etc.\r\n",
        "\r\n",
        "# example of each of the four disaster events in the datasets\r\n",
        "# The eruption of Taal Volcano in Batangas, Philippines\r\n",
        "#data[['text', 'target']][data.index.isin([2998, 3264, 3290, 4110, 4140])]\r\n",
        "\r\n",
        "# Coronavirus\r\n",
        "#data[['text', 'target']][data.index.isin([7975, 7995, 8290, 8323])]\r\n",
        "\r\n",
        "# Bushfires in Australia\r\n",
        "#data[['text', 'target']][data.index.isin([453, 460, 471, 474, 2392])]\r\n",
        "\r\n",
        "# Iran downing of the airplane flight PS752\r\n",
        "data[['text', 'target']][data.index.isin([116, 120, 126, 131, 161])]\r\n",
        "\r\n",
        "# Other\r\n",
        "data[['text', 'target']][data.index.isin([11355, 11354, 11287, 11280, 11270])]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11270</th>\n",
              "      <td>Are you trying to wreck me?? üò≠üò≠üò≠</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11280</th>\n",
              "      <td>It's been 1 month since Ryan proposed to me and I'm still an absolute emotional wreck, cannot and will not stop cry‚Ä¶ https://t.co/hPr3rBSmg5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11287</th>\n",
              "      <td>My nigha just called me and said he got in a wreck and his cuz in ICU. Shit I was having a good day. GOD bless my young FAME</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11354</th>\n",
              "      <td>Yeah, proper Liverpool fans wrecked Man City‚Äôs bus and the Heysel Stadium.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11355</th>\n",
              "      <td>\"Trump and Sisi 'rejected foreign exploitation and agreed that parties must take urgent steps to resolve the conflict before‚Ä¶</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                               text  target\n",
              "11270                                                                                                              Are you trying to wreck me?? üò≠üò≠üò≠       1\n",
              "11280  It's been 1 month since Ryan proposed to me and I'm still an absolute emotional wreck, cannot and will not stop cry‚Ä¶ https://t.co/hPr3rBSmg5       1\n",
              "11287                  My nigha just called me and said he got in a wreck and his cuz in ICU. Shit I was having a good day. GOD bless my young FAME       1\n",
              "11354                                                                    Yeah, proper Liverpool fans wrecked Man City‚Äôs bus and the Heysel Stadium.       1\n",
              "11355                 \"Trump and Sisi 'rejected foreign exploitation and agreed that parties must take urgent steps to resolve the conflict before‚Ä¶       1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "tnhtHt7hibXm",
        "outputId": "15210812-0cd3-4156-a2f7-0984b8613855"
      },
      "source": [
        "### File train.csv\r\n",
        "### show examples of different disaster tweets, non-disaster tweets, strange data, etc.\r\n",
        "\r\n",
        "# example of each of the four disaster events in the datasets\r\n",
        "# Keyword: demolition\r\n",
        "data[['text', 'target']][data.id.isin([3379, 3368, 3365, 3397, 3401])]\r\n",
        "\r\n",
        "# Keyword: explosion\r\n",
        "data[['text', 'target']][data.id.isin([4957, 4980, 4962, 4968, 4988])]\r\n",
        "\r\n",
        "# Keyword: landslide\r\n",
        "data[['text', 'target']][data.id.isin([6675, 6686, 6683, 6681, 6690])]\r\n",
        "\r\n",
        "# Other: wrong manual classification\r\n",
        "data[['keyword', 'text', 'target']][data.id.isin([3369, 3398, 4966, 6654])]\r\n",
        "\r\n",
        "# Other: incomplete tweet\r\n",
        "data[['text', 'target']][data.id.isin([4528, 5650])]\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3154</th>\n",
              "      <td>Setting Up An Emergency Fund In 3 Easy Steps: You never know when a surprise expense will pop up. So work up t... http://t.co/Iz17kLelZC</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3974</th>\n",
              "      <td>#flood #disaster Burst Water Pipe Floods Apartments at NYCHA Senior Center - NY1: NY1Burst Water Pipe Floods A... http://t.co/w7SIIdujOH</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                          text  target\n",
              "3154  Setting Up An Emergency Fund In 3 Easy Steps: You never know when a surprise expense will pop up. So work up t... http://t.co/Iz17kLelZC       0\n",
              "3974  #flood #disaster Burst Water Pipe Floods Apartments at NYCHA Senior Center - NY1: NY1Burst Water Pipe Floods A... http://t.co/w7SIIdujOH       1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i13Jwq9h5kMu",
        "outputId": "b9258cee-2c89-4e0d-daff-de84c9ccc9d9"
      },
      "source": [
        "### examples of hashtag split\r\n",
        "print('#PantherAttack')\r\n",
        "print(wordninja.split('#PantherAttack'), '\\n')\r\n",
        "\r\n",
        "print('#kidsthesedays')\r\n",
        "print(wordninja.split('#kidsthesedays'), '\\n')\r\n",
        "\r\n",
        "print('#LegionnairesDisease')\r\n",
        "print(wordninja.split('#LegionnairesDisease'), '\\n')\r\n",
        "\r\n",
        "print('#FlavorChargedTea')\r\n",
        "print(wordninja.split('#FlavorChargedTea'), '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#PantherAttack\n",
            "['Panther', 'Attack'] \n",
            "\n",
            "#kidsthesedays\n",
            "['kids', 'these', 'days'] \n",
            "\n",
            "#LegionnairesDisease\n",
            "['Legionnaires', 'Disease'] \n",
            "\n",
            "#FlavorChargedTea\n",
            "['Flavor', 'Charged', 'Tea'] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRvM_Nk_6X84"
      },
      "source": [
        "### examples of punctuation & stop words\r\n",
        "print('Punctuation: ', punctuation, '\\n')\r\n",
        "\r\n",
        "print('Stop words:', len(stopwords.words('english')))\r\n",
        "stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "oIj0Yz9bB_XR",
        "outputId": "906b7db9-3097-42f2-fc61-eaaa2004facc"
      },
      "source": [
        "# examples of stemming & lemmatization\r\n",
        "tp = TweetPreprocessing(word_normalization='pos_lemmatization')\r\n",
        "d = tp.preprocess_dataset(data.head())\r\n",
        "data_tokenized = [(row['id'], tp.preprocess_document(row['text_preprocessed']), row['target']) for index, row in d.iterrows()]\r\n",
        "print(data_tokenized)\r\n",
        "d"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(1, ['deed', 'reason', 'earthquake', 'may', 'allah', 'forgive', 'u'], 1), (4, ['forest', 'fire', 'near', 'la', 'ronge', 'sask', 'canada'], 1), (5, ['resident', 'ask', 'shelter', 'place', 'notify', 'officer', 'evacuation', 'shelter', 'place', 'order', 'expect'], 1), (6, ['13,000', 'people', 'receive', 'wildfire', 'evacuation', 'order', 'california'], 1), (7, ['get', 'send', 'photo', 'ruby', 'alaska', 'smoke', 'wildfire', 'pour', 'school'], 1)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>text_preprocessed</th>\n",
              "      <th>duplicate_row</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
              "      <td>1</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
              "      <td>1</td>\n",
              "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
              "      <td>1</td>\n",
              "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
              "      <td>1</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ... duplicate_row\n",
              "0   1  ...         False\n",
              "1   4  ...         False\n",
              "2   5  ...         False\n",
              "3   6  ...         False\n",
              "4   7  ...         False\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC0-VveAUI01"
      },
      "source": [
        "# examples of pos tagging\r\n",
        "document = 'All residents asked to ''shelter in place'' are being notified by officers. No other evacuation or shelter in place orders are expected'\r\n",
        "doc_no_contractions = contractions.fix(document)\r\n",
        "words = tp.tokenizer.tokenize(doc_no_contractions)\r\n",
        "doc_no_punct = [w.lower() for w in words if w not in punctuation]      \r\n",
        "doc_no_hashtags = tp.unnest_lists([wordninja.split(w[1:]) if w.startswith('#') else w for w in doc_no_punct])\r\n",
        "tokens_to_ignore = tp.stop_words.union(set(tp.ignore_words))\r\n",
        "[d for d in doc_no_hashtags if d not in tokens_to_ignore]\r\n",
        "[d for d in nltk.pos_tag(doc_no_hashtags) if d[0] not in tokens_to_ignore]\r\n",
        "[d for d in tp.get_document_pos(doc_no_hashtags) if d[0] not in tokens_to_ignore]\r\n",
        "tp.get_document_pos(doc_no_hashtags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqfV9jUwN3xe"
      },
      "source": [
        "## Split data to train and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfrzjP5H0cm2"
      },
      "source": [
        "seed = 1\n",
        "data_shuffled = data.iloc[np.random.RandomState(seed=seed).permutation(len(data))]\n",
        "\n",
        "train_fraction = 0.9\n",
        "split_point = int(train_fraction *len(data_shuffled))\n",
        "train_data = data_shuffled[0:split_point]\n",
        "test_data = data_shuffled[split_point:]\n",
        "\n",
        "tp = TweetPreprocessing(word_normalization='stemming')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnIGN5wumHbF"
      },
      "source": [
        "###Apply feature ranking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDk7N80imBBG"
      },
      "source": [
        "fr = FeatureRanking(tp)\r\n",
        "fr.calculate(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Puzt5chIO4fu"
      },
      "source": [
        "### Initialize classifier and train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx0tXGw9lJko"
      },
      "source": [
        "vocab = None\n",
        "#vocab = sorted(fr.mi_rank, key=fr.mi_rank.get, reverse=True)[:1000]\n",
        "#vocab = sorted(fr.mi_rank, key=fr.mi_rank.get, reverse=True)[:5000]\n",
        "#vocab = sorted(fr.mi_rank, key=fr.mi_rank.get, reverse=True)[:10000]\n",
        "\n",
        "#vocab = sorted(fr.chi_sq_rank, key=fr.chi_sq_rank.get, reverse=True)[:1000]\n",
        "#vocab = sorted(fr.chi_sq_rank, key=fr.chi_sq_rank.get, reverse=True)[:5000]\n",
        "#vocab = sorted(fr.chi_sq_rank, key=fr.chi_sq_rank.get, reverse=True)[:10000]\n",
        "\n",
        "nb = NaiveBayesDocumentClassifier(tp, vocabulary=vocab)\n",
        "nb.fit(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSXvVLZ4O9Po"
      },
      "source": [
        "### Initialize document search object and fit to create index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-YeE48_ry2g"
      },
      "source": [
        "ds = DocumentSimilaritySearch()\r\n",
        "preprocessed_data = tp.preprocess_dataset(data)\r\n",
        "ds.create_index(preprocessed_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23juDqD3sXlN",
        "outputId": "d139f3ce-552a-4052-af3f-1a2f2caba3bc"
      },
      "source": [
        "ds.find_similar_documents('Landslide in Hong Kong', 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " ...\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4678                                                                                         @kemal_atlay caught in a landslide\n",
              "4704                                                                                      #landslide while on a trip in #skardu\n",
              "4705                                                                               Listen to Landslide by Oh Wonder #SoundCloud\n",
              "4703    So when you're caught in a landslide\\nI'll be there for you\\nAnd in the rain \\ngive you sunshine\\nI'll be there for you\n",
              "5577                                                              Landslide caused by severe rainstorm kills 3 in Italian√•√äAlps\n",
              "Name: text_preprocessed, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du9kqSLEPA7e"
      },
      "source": [
        "### Use classifier to make predictions on validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpiQw7Ldgtl7",
        "outputId": "b887321c-ab14-40ff-9f91-034a17b548e5"
      },
      "source": [
        "d = test_data.copy()\n",
        "d['predict'] = d['text'].apply(nb.predict)\n",
        "\n",
        "y_true = d['target']\n",
        "y_pred = d['predict']\n",
        "\n",
        "cs = ClassificationStats(y_true, y_pred)\n",
        "print('Confusion matrix\\n', cs.cm)\n",
        "print('Precision: ', cs.precision())\n",
        "print('Recall: ', cs.recall())\n",
        "print('F-score ', cs.f_measure())\n",
        "print('Accuracy ', cs.accuracy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix\n",
            " [[377  56]\n",
            " [ 82 247]]\n",
            "Precision:  0.8151815181518152\n",
            "Recall:  0.7507598784194529\n",
            "F-score  0.7816455696202531\n",
            "Accuracy  0.8188976377952756\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rxo7mn7APOqW"
      },
      "source": [
        "### Plot precision-recall curve for results on validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "Qti8ITzY5uVe",
        "outputId": "2290cb2a-de77-4044-ab82-b260a6e3f7c7"
      },
      "source": [
        "from matplotlib import pyplot\r\n",
        "from matplotlib.pyplot import figure\r\n",
        "\r\n",
        "fig = figure(num=None, figsize=(7, 5), dpi=80, facecolor='w', edgecolor='k')\r\n",
        "fig.suptitle('Precision Recall Curve', fontsize=14)\r\n",
        "\r\n",
        "d = test_data.copy()\r\n",
        "d['predict_probs'] = d['text'].apply(nb.predict_proba)\r\n",
        "\r\n",
        "y_true = d['target']\r\n",
        "y_pred_probs = [dd[1] for dd in d['predict_probs']]\r\n",
        "\r\n",
        "precision, recall, thresholds = precision_recall_curve(y_true, y_pred_probs)\r\n",
        "pyplot.plot(recall, precision, marker='.', label='NB')\r\n",
        "pyplot.xlabel('Recall', fontsize=13)\r\n",
        "pyplot.ylabel('Precision', fontsize=13)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Precision')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 560x400 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWt-5NsyPs5G"
      },
      "source": [
        "### Show the words/tokens with highest probability to occur"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBnYHQRRflfG",
        "outputId": "03e2772a-0ecd-4c13-aabb-cc8ea26fe335"
      },
      "source": [
        "disaster_words = [(w[0], nb.P_words[w]) for w in nb.P_words if w[1] == 1]\r\n",
        "non_disaster_words = [(w[0], nb.P_words[w]) for w in nb.P_words if w[1] == 0]\r\n",
        "disaster_words.sort(key=(lambda x: x[1]), reverse=True)\r\n",
        "non_disaster_words.sort(key=(lambda x: x[1]), reverse=True)\r\n",
        "\r\n",
        "print('Most common words from disaster tweets')\r\n",
        "print(disaster_words[0:10])\r\n",
        "print('Most common words from non disaster tweets')\r\n",
        "print(non_disaster_words[0:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most common words from disaster tweets\n",
            "[('fire', 0.00575363566348027), ('news', 0.004258596475331854), ('kill', 0.0032392515743215693), ('bomb', 0.0027409051782720972), ('get', 0.002491731980247361), ('via', 0.0024464277624246817), ('disaster', 0.0024464277624246817), ('suicide', 0.002355819326779323), ('storm', 0.0022878630000453043), ('people', 0.0022652108911339645)]\n",
            "Most common words from non disaster tweets\n",
            "[('get', 0.005765547827839936), ('like', 0.005362362665053926), ('go', 0.004334240499949602), ('new', 0.0033867553674024795), ('would', 0.002519907267412559), ('one', 0.002499748009273259), ('make', 0.0024594294929946576), ('want', 0.0023989517185767566), ('body', 0.002217518395323052), ('love', 0.0021167221046265497)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WK4mFZNQZcf"
      },
      "source": [
        "### Apply Naive Bayes from sklearn for comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRJTBoEa0l0L"
      },
      "source": [
        "import sklearn\r\n",
        "import sklearn.naive_bayes\r\n",
        "\r\n",
        "train_data_pp = tp.preprocess_dataset(train_data)\r\n",
        "train_data_pp['tokenized_text'] = train_data_pp.text_preprocessed.apply(tp.preprocess_document).apply(lambda x: ' '.join(x))\r\n",
        "# Convert to bag of words\r\n",
        "count_vect = sklearn.feature_extraction.text.CountVectorizer(vocabulary=nb.dictionary)\r\n",
        "X = count_vect.fit_transform(train_data_pp.tokenized_text)\r\n",
        "\r\n",
        "# Convert from occurrences to frequencies\r\n",
        "# Occurrence count is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\r\n",
        "# To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies.\r\n",
        "transformer = sklearn.feature_extraction.text.TfidfTransformer()\r\n",
        "X = transformer.fit_transform(X)\r\n",
        "\r\n",
        "# Create a model\r\n",
        "model = sklearn.naive_bayes.MultinomialNB(alpha=0.3, fit_prior=True, class_prior=None)\r\n",
        "# Train the model\r\n",
        "model.fit(X, train_data_pp.target)\r\n",
        "\r\n",
        "predictions = model.predict(X)\r\n",
        "accuracy = sklearn.metrics.accuracy_score(train_data_pp.target, predictions)\r\n",
        "print('Accuracy: {0:.2f}'.format(accuracy * 100.0))\r\n",
        "\r\n",
        "test_data_pp = tp.preprocess_dataset(test_data)\r\n",
        "test_data_pp['tokenized_text'] = test_data_pp.text_preprocessed.apply(tp.preprocess_document).apply(lambda x: ' '.join(x))\r\n",
        "\r\n",
        "X = count_vect.transform(test_data_pp.tokenized_text)\r\n",
        "X = transformer.transform(X)\r\n",
        "predictions = model.predict(X)\r\n",
        "accuracy = sklearn.metrics.accuracy_score(test_data_pp.target, predictions)\r\n",
        "print('Accuracy: {0:.2f}'.format(accuracy * 100.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ung-eUM2Efea"
      },
      "source": [
        "consumerKey = '7gpSS06BU42kzp82QSh4dln5w'\r\n",
        "consumerSecret = '7r8INlF8yjwDe8UmhklTXeGOF5L1YmEtS72NZtKOmbjfGTL94n'\r\n",
        "token = '1273581727223554049-ruPO5HygK6CehA0rF98l1uMe8fJnaE'\r\n",
        "secret = 'Gcdi8yOEHOwMBkN89AZr2WKVSqhdw3IYoZzVMTWRYWaRv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5qEgYjqP3Aw"
      },
      "source": [
        "## Run Twitter stream with classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3K2dUwUzTml",
        "outputId": "bb7d7100-ff76-4081-9d4f-3cf9736c23d7"
      },
      "source": [
        "# Authenticate to Twitter\r\n",
        "auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\r\n",
        "auth.set_access_token(token, secret)\r\n",
        "\r\n",
        "# Create API object\r\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\r\n",
        "\r\n",
        "tweets_listener = TwitterClassificationStreamListener(api, time_limit=5, classifier=nb, prob_threshold=0.8)\r\n",
        "stream = tweepy.Stream(api.auth, tweets_listener)\r\n",
        "\r\n",
        "tokens_to_track = [d[0] for d in disaster_words[0:5] if d[0] not in [nd[0] for nd in non_disaster_words[0:10]]]\r\n",
        "stream.filter(track=tokens_to_track, languages=['en'])\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RT @finebrownthang: @hecraveskay Nope. They start threatening to kill you &amp; themselves üòÇü§∑üèæ‚Äç‚ôÄÔ∏è\n",
            "RT @NEWSTALK1010: \"He made a mistake\" - Toronto Mayor says of travelling finance minister #onpoli #TOpoli https://t.co/POKXQKDbV6 https://t‚Ä¶\n",
            "RT @imMAK02: One more encounter in Kashmir.\n",
            "\n",
            "3 Kashmiri youth were killed by government forces in Srinagar Kashmir.\n",
            "\n",
            "Families said they wer‚Ä¶\n",
            "RT @Meidas_Kelly: So 1 day into my vacation and all I see is news about COVID. Death tolls, LA has no beds and 600 new cases an hour. At le‚Ä¶\n",
            "RT @Resistance411: @JennaEllisEsq Never forget that Trump has hundreds of millions in loans from the state owned Bank of China üá®üá≥, and Ivan‚Ä¶\n",
            "RT @MikayesFiona: BREAKING NEWS\n",
            "\n",
            "SENATOR JOSH HAWLEY PUBLICLY STATES HE WILL OBJECT TO ELECTORS ON JAN 6!\n",
            "\n",
            "ALL WE NEEDED WAS 1 AND WE HAVE‚Ä¶\n",
            "RT @GuardianNigeria: Yobe earmarks N11.7b for road, market projects | The Guardian Nigeria News - Nigeria and World News https://t.co/fN0Yc‚Ä¶\n",
            "RT @gkatz: Women often warn us about violent men. When will we listen? | Nashville explosion: Woman warned MNPD Warner was building bomb in‚Ä¶\n",
            "RT @wFalabede: in September 2002\n",
            "\n",
            "Barnabas Igwe a critic of the OBJ government who is also chairman of the state branch of the Nigerian\n",
            "Bar‚Ä¶\n",
            "RT @CrystalCarterL: üö®Georgia Sen Judiciary Committee üö® \n",
            "\n",
            "voted to allow #FultonCounty absentee ballots to be fully audited by @JovanHPulitz‚Ä¶\n",
            "RT @shen_liya: @drmacavinta @heynowhank22 @AaronBlake But that is not intentional but due to systemic smearing campaign by the Fake News. O‚Ä¶\n",
            "RT @lmorphine20: Presave KILL 3 : https://t.co/im0QA2tdHp\n",
            "The latest Daily Space News! https://t.co/85qdz3NliI Thanks to @wings_around #satellites #science\n",
            "RT @TheSteffie: I firmly believe Chopper has a higher kill count than Darth Vader. \n",
            "\n",
            "Chopper wakes up and chooses violence. Darth Vader is‚Ä¶\n",
            "RT @Mixmag: Palestinian DJ Sama‚Äô Abdulhadi has been arrested for playing an event at a historical site near the West Bank city of Jericho‚Ä¶\n",
            "RT @LBC: Buckinghamshire has declared a ‚Äúmajor incident\" as coronavirus cases surge, just hours after Essex requested military help to cons‚Ä¶\n",
            "RT @ElizLanders: That $600 stimulus is going out starting tonight: ‚ÄúThe initial direct deposit payments may begin arriving as early as toni‚Ä¶\n",
            "RT @critpraxis: @PhilGreaves01 People have very short memories, even MSM reported a decade ago that swine flu was a complete hoax  fabricat‚Ä¶\n",
            "RT @krishgm: 981 new covid deaths recorded today and over 50k cases again. Despite the vaccine news this is not a day to celebrate.\n",
            "RT @HindustanTimes: Dug up stretch around Narwana Road triggers traffic chaos, irks locals\n",
            "https://t.co/sovJyV7sZt\n",
            "RT @CrystalCarterL: üö®Georgia Sen Judiciary Committee üö® \n",
            "\n",
            "voted to allow #FultonCounty absentee ballots to be fully audited by @JovanHPulitz‚Ä¶\n",
            "RT @Sromines: Firing 16 shots as suppressive fire and killing an unarmed bystander without ‚Äúproperly identifying a target‚Äù is practically t‚Ä¶\n",
            "Feds probing if Nashville bomber believed in lizard people conspiracy (NBC News)\n",
            "\n",
            "https://t.co/xNBal6x02L\n",
            "https://t.co/YRJyXsii0j\n",
            "‚ÄòOverwhelming combat power‚Äô: U.S. bombers fly over Persian Gulf in clear warning to Iran via @washtimes‚Ä¶ https://t.co/hNeNSoDXtu\n",
            "RT @EllaCoreleone: Great news from Georgia - Fulton county must turn over all ballots counted after 11 pm  Nov. 3. GA electors have to be o‚Ä¶\n",
            "RT @AhtshamOffical: #RawBehindKarimaDemise\n",
            "The incident served as a trigger for the Indian media, which squarely blamed Pakistan for her de‚Ä¶\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aL0qeXlsh7_"
      },
      "source": [
        "query = [t[0] for t in tweets_listener.tweets_list if t[1] == 1]][0]\r\n",
        "print(query)\r\n",
        "#ds.find_similar_documents(query, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OApTmh-sQAFw"
      },
      "source": [
        "## Load test dataset and apply classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgUzHd20EFqT"
      },
      "source": [
        "# make predictions on test.csv\r\n",
        "kaggle_test = pd.read_csv('/content/sample_data/test.csv')\r\n",
        "kaggle_test['predict'] = kaggle_test['text'].apply(nb.predict)\r\n",
        "kaggle_test[['id', 'predict']].to_csv(r'/content/sample_data/test_pred.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}